---
title: "Multi-Class Classification Problem"
author: "Andriy Fedorenko"
date: "Friday, February 16, 2015"
output:
 pdf_document: default
 html_document:
  keep_md: yes
 word_document: default
---
```{r include=FALSE}
#Data Cleaning section:
#the following set of packages were used  
AddNamesOfColl<-function(dat,start,end){ 
        tem<-as.character()
        namesOfVaribales<-names(dat)
        for (i in start:(end-1)) {tem<-paste(tem,namesOfVaribales[i],"+")}
        tem<-paste(tem,namesOfVaribales[end])
        return((tem))
        }


LoadMyPackages<-function() {
        library(caret)
        library(ElemStatLearn)
        library(ggplot2)
        library(rpart)
        library(kernlab)
        library(Hmisc)
        library(e1071)
        library(MASS)
        library(ISLR)
        library(boot)
        library(rmarkdown)
        library(knitr)
        library(devtools)
        }

LoadMyPackages()
    
```

##Introduction

<p style='text-align: justify;'> 
The data were collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.The goal of this project is to test different statistical approaches on the dataset, find the best conditions for data prediction based on test error estimates for multi-class classification problem and predict the manner in which volunteers did the exercises. This is the "classe" variable in the training set. The report describs how the model was build, how a cross validation was used, how many parameters needed to get minimum test error. The model also predicts 20 different test cases. More information is available from the website here:
http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</p>

##Solution: 
```{r echo=FALSE}

svmtest<-function(cleaned.data,new.raw.data){
        mydata<-new.raw.data
        require(e1071)
        #removed variables containing more than 80% of NA's
        pp<-DeteidexofNas(mydata)
        ppst1<-mydata[,-pp]
        
        #No Zeroes Variance Variable
        nzv <- nearZeroVar(ppst1)                   
        ppst2<-ppst1[,-nzv]
        
        ppst4<-ppst2[,-c(2,5)]  
        
        #No Characters
        temp=dim(ppst4)[2]
        ppst5<-ChangeClassforMatrix(ppst4,1,temp-1,"numeric")       
        
        #make all columns as numerical
        temp0=dim(ppst5)[2]
        ppst5[,temp0]<-as.factor(ppst5[,temp0])           
        
        #make the last columns as factor
        temp1=dim(ppst5)[2]
        ppst6<-preProcess(ppst5[,-temp1],method=c("scale","center")) #Scale and Center
        ppst7<-predict(ppst6,ppst5[,-temp1])
        
        end.col<-dim(cleaned.data)[2]
        model <- svm(cleaned.data$classe~., cleaned.data[,-end.col] )
        
        train<- predict(model, newdata=cleaned.data)
        res<-predict( model, newdata=ppst7)
        
        conf.mat<-confusionMatrix(train,cleaned.data$classe)
        return(list(res=res,conf.mat=conf.mat,model=model,ppst7=as.data.frame(ppst7)))
        }

new.data<-read.csv('pml-testing.csv')
raw.data<-read.csv("pml-training.csv",header=TRUE,na.strings=c("NA","#DIV/0!", ""),stringsAsFactors=FALSE)

```
<p style='text-align: justify;'> 
```{r}
dim(raw.data)
```
</p>

<p style='text-align: justify;'> 
The quality of work-out in each exercise was tested based on 159 parameters and 19622 experimental cases (or samples) and there are five different categories that can be assigned to each case depending on how well exercise was performed by a volunteer. To solve such multi-class classification problem, I used generalized linear model to predict the accuracy test error for the dataset. Before we can start modeling, let's take a look at our data.</p>

##Data Cleaning:

<p style='text-align: justify;'> 
From the tables below we can find that data frame in the training set contains a raw data. For instance, $var_yaw_arm variable has more than 80 present of NAs.</p>
```{r echo=FALSE}
str(raw.data$skewness_roll_dumbbell)
```
<p style='text-align: justify;'> 
Some of the variables, like $pitch_belt or $yaw_arm, have very high standard deviation compares to the mean value, suggesting that most of the values of such variables are small, but there are few that are much larger. The distribution, in this case, is skewed and the tested model may not converge properly on this dataset.  
</p>
```{r include=FALSE}
mean.values<-as.matrix(lapply(raw.data,mean))
sd.values<-as.matrix(lapply(raw.data,sd))
data.properties<-data.frame((mean.values),(sd.values))
```

```{r fig.width=3.4, fig.height=3.7, echo=FALSE}
data.properties[c(9,48),]
hist(raw.data$pitch_belt,main ="pitch_belt")
hist(raw.data$yaw_arm,main ="yaw_arm,main")
```
<p style='text-align: justify;'> 
Also, the variables have different types so that in order for gml model to converge and predict testing set reliably, all raw data variables have to be standardized.</p>

```{r echo=FALSE}
str(raw.data[c(11,6,9,14)])
```

```{r echo=FALSE}
my.dummy.variable<-function(data,IndexColToBeDummy){ 
        
        #make dummy variables from one factor column 
        class.name<-sapply(data,class)
        index.classVar<-which(class.name=="factor") #index of factor columns
        temp.vector<-data[,index.classVar] #get the factor colum and assing it to vector
        if (index.classVar!=IndexColToBeDummy) stop("type correct column to be dummied")     
        
        UniqueLevels<-unique(data[,index.classVar]) #unique levels(velues) in factor colunm
        data<-data[,-index.classVar]
        tem<-data.frame()
        tem1<-data.frame()
        tem2<-data.frame()
        
        for (i in 1:length(UniqueLevels)) { #create a dummy varibale matrix
                rr<-temp.vector==UniqueLevels[i]
                rr1<-as.numeric(rr)
                #rr1<-as.factor(rr1)
                tem<-rbind(tem,rr1)
                }
        tem1<-t(tem)
        tem2<-data.frame(tem1)
        names(tem2)<- UniqueLevels #assingin names for colums
        data<-data[,-index.classVar]
        #done<-cbind(data,tem,deparse.level=0)
        
        return(list(Num.data=data,DummyVariables=tem2))
        }
#returns the idecies (df_Ind) and detects the names (df) of colums where NA's were more than 80 precent
NAVariable<-function(data){ 
        df<-c()
        df_Ind<-c()
        for (i in 1:dim(data)[2]) {
                if ((length(which(!is.na(data[,i])))/length(data[,i]))<0.8)
                        {df<-cbind(df,names(data)[i])
                         df_Ind<-cbind(df_Ind,i)
                         } 
                } 
        
        return(df_Ind)
        }


#change the classes for data variables starting from StartPos to EndPosition 
#the value of the last variable NameOfTheClassToChangeTo has to be in ""
ChangeClassforMatrix<-function(data,StartPos,EndPosition,NameOfTheClassToChangeTo){ 
        for (i in StartPos:EndPosition) {
                if (NameOfTheClassToChangeTo=="numeric") {data[,i]<-as.numeric(data[,i])}
                if (NameOfTheClassToChangeTo=="factor") {data[,i]<-as.factor(data[,i])}
                } 
        return(data)
        }

correlation.pca.data.cleaning<-function(ppst7) {
        endcol<-dim(ppst7)[2]
        ppst8_1<-abs(cor(ppst7[,-endcol]))               
        ppst8_2<-findCorrelation(ppst8_1,cutoff = 0.9)
        ppst8<-ppst7[,-ppst8_2]
        return(ppst8) 
        }

DeteidexofNas<-function(data){ 
        #detect the idecies (df_Ind) and names (df) of colums where NA's are of more than 80 precent
        df<-c()
        df_Ind<-c()
        for (i in 1:dim(data)[2]) {
                if ((length(which(!is.na(data[,i])))/length(data[,i]))<0.8)
                        {df<-cbind(df,names(data)[i])
                         df_Ind<-cbind(df_Ind,i)
                         } 
                } 
        
        return(df_Ind)
        }


data.cleaning<-function(mydata,number.sampled.data){
        
        #removed variables containing more than 80% of NA's
        pp<-DeteidexofNas(mydata)
        ppst1<-mydata[,-pp]
        
        #No Zeroes Variance Variable
        nzv <- nearZeroVar(ppst1)          
        ppst2<-ppst1[,-nzv]
        
        #No Zeroes, complete samples
        ppst3<-subset(ppst2,ppst2$pitch_forearm!=0) 
        ppst4<-ppst3[,-c(2,5)]           
        
        #No Characters
        temp=dim(ppst4)[2]
        ppst5<-ChangeClassforMatrix(ppst4,1,temp-1,"numeric")    
        
        #make all columns as numerical
        temp0=dim(ppst5)[2]
        ppst5[,temp0]<-as.factor(ppst5[,temp0])      
        
        #make the last columns as factor
        temp1=dim(ppst5)[2]
        ppst6<-preProcess(ppst5[,-temp1],method=c("scale","center")) #Scale and Center
        ppst7<-predict(ppst6,ppst5[,-temp1])
        
        #Scale and Center
        ppst7[, "classe"] <- ppst5$classe
        
        set.seed(1)
        
        dd1<-ppst7[sample(nrow(ppst7),number.sampled.data),]
        
        return(dd1) #No Correaltion on the data 
        
        
        }

cleaned.data<-data.cleaning(raw.data)
new.dataa<-svmtest(cleaned.data,new.data)[[4]]
dummy.num.data<-my.dummy.variable(cleaned.data,57)
```
<p style='text-align: justify;'>
In order to standardize the variables, the raw data were preprocessed with data.cleaning() and my.dummy.variable() functions that performed following procedures:

* removed variables containing more than 80% of NA's 
* removed Near Zero-Variance predictors 
* made all cases complete 
* removed all character and date variables 
* made all left variables as numeric
* made the outcome variable as a factor 
* scale and center all variables 
* created a dummy variables from outcome, where each dummy variable corresponds to one of the categories of multi-class vector, such as "A", "B",.,"E".</p>

<p style='text-align: justify;'>
The final preprocessed data is the list of 56 numerical variables and 5 dummy variables corresponding to each of five categories "A", "B",.,"E". </p>
```{r echo=FALSE}
summary(dummy.num.data)
```

##Results:
## Tuning the number of samples of the tidy data.
<p style='text-align: justify;'> 
 To predict twenty samples of the test set, we need first of all to optimize the parameters of glm model and find out optimal conditions which we will use for our prediction. We could fit the model to the available preprocessed data, choose a set of parameters of the model that will correspond to the minimum of accuracy error (or misclassified samples) and predict the final outcome of the test set using these parameters. But this method would have suffered a high bias since the model may fit well on preprocessed data but may fail to fit new set of data that were not present in fit dataset. The main goal is to predict how well the model will fit the new, unseen data set. To investigate the accuracy error of the model on unseen data, the better strategy would be to split the preprocessed data into at least two separate parts, fit the model to the first part and then using received parameters to find the minimum of the accuracy error on the second part of the data. This approach will give us the clearer picture as to how well the glm model will behave on new data. To implement such approach, I partitioned the preprocessed data into two separate parts (training and testing sets) and investigated the behavior of both train and test errors as the function of the percentage of data that goes to training set.</p>
```{r echo=FALSE, comment=FALSE, warning=FALSE}

AddNamesOfColl<-function(dat,start,end){ 
        tem<-as.character()
        namesOfVaribales<-names(dat)
        for (i in start:(end-1)) {tem<-paste(tem,namesOfVaribales[i],"+")}
        tem<-paste(tem,namesOfVaribales[end])
        return((tem))
        }

```

```{r echo=c(272), comment=FALSE, warning=FALSE}    
#MachineLeariningProjectVarianceTest uses glm object and "OneVsAll" approach for all factor dummy variables and 
#returns the mean value for train and test sets among all (in or case 5) categories. 
    
MachineLeariningProjectVarianceTest<-function(data,dummyvar,Cv_Value){
        #how Error depens on the number of samples?
        #glm: fitting of data with factor vector dummyvar containing n colums of factors for multi-class classification problem.
        #Cv_Value, presentage of data suppose to be Training set, a "p" number in createDataPartition
        
        dummyVarlenght<-dim(dummyvar)[2]
        dataEndVal<-dim(data)[2]
        modelFormula<-AddNamesOfColl(data,1,dataEndVal)
        TrainAcuracy<-vector()
        TestAcuracy_ts<-vector()
        for (i in 1:dummyVarlenght) {
                TempMatrix<-cbind(data,dummyvar[i])
                indx<-dim(TempMatrix)[2]
                dataEndVal1<-dim(TempMatrix)[2]
                
                
                inTrain<-createDataPartition(y=TempMatrix[,indx],p=Cv_Value,list=FALSE)
                training<-TempMatrix[inTrain,]
                testing<-TempMatrix[-inTrain,]
                
                UpdateFit<-paste(names(training[dataEndVal1]),"~",modelFormula)
                glm.training<-glm(UpdateFit,data=training,family=binomial)
                glm.fit<-glm(UpdateFit,data=training,family=binomial)
                
                glm.summary<-glm.fit
                
                glm.probs_x<-predict(glm.fit,newdata=training,type="response")
                glm.pred_Train_x<-ifelse(glm.probs_x>0.5,"1","0")
                CFglm.pred_Train_x<-confusionMatrix(glm.pred_Train_x,training[,indx])
                TrainAcuracy<-cbind(TrainAcuracy,CFglm.pred_Train_x[[3]][[1]])
                
                glm.probs_ts<-predict(glm.training,type="response",newdata=testing)
                glm.pred_ts<-ifelse(glm.probs_ts>0.5,"1","0") 
                CFglm.pred_ts<-confusionMatrix(glm.pred_ts,testing[,indx]) 
                TestAcuracy_ts<-cbind(TestAcuracy_ts,CFglm.pred_ts[[3]][[1]])
                
                
                }
        MachineLeariningProjectVarianceTest<-list(TrainAcuracy=mean(TrainAcuracy),TestAcuracy_ts=mean(TestAcuracy_ts),glm.summary=glm.summary)
        return(MachineLeariningProjectVarianceTest)
        
        
        }

MachineLeariningProject_Error_Testing<-function(data,factors,Cv_Val){ 
        #data-numeric data, factors-data.frame of factors recived from MyDummy, Cv_Val step presentage of data to be investigated 
        #numeric Data to fit model to, factor vector containing n numbers of caregories for multi class classification problem "A" "B",..,"E" ect.
        #together with MachineLeariningProjectVarianceTest
        
        train_error<-vector()
        test_error<-vector()
        x<-seq(from=0.001,to=1,by=Cv_Val)
        for (i in x){
                temp<-MachineLeariningProjectVarianceTest(data,factors,i)
                train_error<-cbind(train_error,temp[[1]])
                test_error<-cbind(test_error,temp[[2]])
                
                }
        
        glm.fit<-temp[[3]]
        t<-summary(glm.fit)
        tt<-t$coefficients
        ttt<-which(tt[,4]>=0.7)
        glm.sumamry<-tt[ttt,]
        
        
        
        p<-plot(x,1-train_error,type="b",col=2,xlim=c(0.0001,1),ylim=c(0,0.6),xlab="The Fraction Of pre-Processed Data",ylab="Error",main="",cex.lab=1, cex.axis=1,cex.main=1,cex=0.7)
        
        par(new=T)
        
        plot(x,1-test_error,col="blue",cex=0.7,axes=F,ann=F,type="b",pin=c(6,4),ylim=c(0,0.5),xlim=c(0.0001,1))
        text(c(0.8,0.8),c(0.45,0.4),labels=c("Test Error","Train Error"),col=c("blue","red"), adj=c(0,0))
        
        #plot(x,1-test_error,col="blue",cex=1,axes=F,ann=F,type="b",mar=c(16, 12, 16, 12),pin=c(8,6),ylim=c(0,0.6),xlim=c(0.0001,1))
        par(new=F)
        return(glm.sumamry)
        #return(list(train_error=train_error,test_error=test_error))
        }


return.glm.summary<-MachineLeariningProject_Error_Testing(dummy.num.data[[1]],dummy.num.data[[2]],0.02)


    
```
<p style='text-align: justify;'> 
It is clear from the figure that the model tends to overfit the training data when fewer samples are used in training set (red line) and that the test error has much more bias having the accuracy error near 0.55 (blue line). The more training data we use to adjust the model parameters the less test error we can observe. When above 20-25% of preprocessed data are used in the training set the test error reaches saturation at 0.045 (or 4.5%), so getting more training data will not help much to improve test error. Thus, more than 4000 samples should be used to train the glm model and to reliably predict a new data set with 4.5% of accuracy error or 95.5% of accuracy.</p>

##K-Fold Cross-Validation:
<p style='text-align: justify;'> 
We can also divide the set of samples into k groups or folds of equal size. The first fold is treated as a test set, and the model is fit on the remaining k-1 folds. The accuracy error is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a training set. This process results in k estimates of the test error and the final error estimate is computed by averaging these k values. 
```{r echo=FALSE, comment=FALSE, warning=FALSE}

CVoneVsAll<-function(data,onefactor,nameFactor,K,EndValueofNumerofVariable,StartValueofNumerofVariable=1){
        
        TrainAcuracy<-vector()
        TestAcuracy_ts<-vector()
        
        TempMatrix<-cbind(data,onefactor)
        EndCol<-dim(TempMatrix)[2]
        EndDataCol<-EndValueofNumerofVariable
        colnames(TempMatrix)[EndCol]<-nameFactor
        
        modelFormula<-AddNamesOfColl(data,StartValueofNumerofVariable,EndDataCol)
        UpdateFit<-paste(colnames(TempMatrix)[EndCol],"~",modelFormula)
        
        flds <- createFolds(TempMatrix[,EndCol], k = K, list = TRUE, returnTrain = T) #make K folds for corss valdation set 
        
        #preform cross validation for range of diffferent K folds starting from 1 to K
        for (i in 1:K) { 
                
                #make training set
                training<-TempMatrix[flds[[i]],] 
                
                #make testing set
                testing<- TempMatrix[-flds[[i]],] 
                
                #fit the glm model for dataset, one dummy column and K folds 
                glm.fit<-glm(UpdateFit,data=training,family=binomial) 
                
                # predition on training set
                glm.probs_x<-predict(glm.fit,newdata=training,type="response")
                
                # making probabilities as binary outcome 
                glm.pred_Train_x<-ifelse(glm.probs_x>0.5,"1","0") 
                
                TrainTable<-table(glm.pred_Train_x,training[,EndCol])
                TrainAcuracy<-cbind(TrainAcuracy,(TrainTable[1,1]+TrainTable[2,2])/sum(TrainTable))
                
                # predition on testing set
                # making probabilities as binary outcome 
                glm.probs_ts<-predict(glm.fit,type="response",newdata=testing) 
                glm.pred_ts<-ifelse(glm.probs_ts>0.5,"1","0")        
                
                
                TestTable<-table(glm.pred_ts,testing[,EndCol])
                # comperaing testing factor with predicted on testing set using table function
                #print(TestTable[2,2])
                TestAcuracy_ts<-cbind(TestAcuracy_ts,(TestTable[1,1]+TestTable[2,2])/sum(TestTable)) 
                
                }
        CVoneVsAll<-list(TrainAcuracy<-mean(TrainAcuracy),TestAcuracy_ts<-mean(TestAcuracy_ts))
        return(CVoneVsAll)
        
        }

CossValidation<-function(data, factors, K){ 
        # K- cross validation with glm starting from fold=2 to K.K will be sent to CVoneVsAll function
        TrainlistNames<-list()
        TestlistNames<-list()
        
        TrainError<-vector()
        TestError<-vector()
        
        NumbrOfVaibale<-dim(data)[2]    
        DimFactors<-dim(factors)[2]
        x=seq(from=2,to=K,by=1) # x vector of K's
        
        for (d in 2:K){ # do cross validation for range of folds numbers K starting from 2
                
                TrainErrortemp<-vector()
                TestErrortemp<-vector()
                TempMatrix<-data.frame() 
                
                for (i in 1:DimFactors){ 
                        # do cross validation for each of the dummy factors column
                        
                        TempMatrix<-cbind(data,factors[i]) #merge data and factor column "A" (i=1), B(i=2),..) 
                        TrainErrortemp<-cbind(TrainErrortemp,CVoneVsAll(data,factors[i],names(factors)[i],d,NumbrOfVaibale)[[1]] )
                        #train Error for every of Factors colmun for "A" (i=1), "B" (i=2),..) 
                        
                        
                        TestErrortemp<-cbind(TestErrortemp,CVoneVsAll(data,factors[i],names(factors)[i],d,NumbrOfVaibale)[[2]])
                        #test Error for every of Factors colmun for "A" (i=1), "B" (i=2),..) 
                        
                        }
                
                TrainlistNames<-c(TrainlistNames,TrainErrortemp)
                TestlistNames<-c(TestlistNames,TestErrortemp)
                
                TrainError<- cbind(TrainError,mean(TrainErrortemp)) #get the mean value all train errors of "A" (i=1), "B" (i=2),..
                TestError<- cbind(TestError,mean(TestErrortemp)) #get the mean value all test errors of "A" (i=1), "B" (i=2),..
                
                }
        
        
        return(list(TrainError= TrainError,TestError=TestError,x=x))
        }


```

```{r fig.width=7, fig.height=7, echo=FALSE, comment=FALSE, warning=FALSE}
make.cv_plots.for.samples.range<-function(raw.data){
        par(mfrow=c(2,2))
        
        #par(mfrow=c(2,2))
        Kfold.range<-c(10,10,10,10)
        k=1
        samples.vector<-c(500,1000,5000,15000)
        for (i in samples.vector) { 
                raw.data.temp<-data.cleaning(raw.data,i)
                dummy.num.data.temp<-my.dummy.variable(raw.data.temp,dim(raw.data.temp)[2])
                temp.cross.validation<-CossValidation(dummy.num.data.temp[[1]],dummy.num.data.temp[[2]],Kfold.range[k])
                
                k<-k+1
                
                TrainError<-temp.cross.validation[[1]]
                TestError<-temp.cross.validation[[2]]
                x<-temp.cross.validation[[3]]
                
                plot(x,1-TrainError,type="b",col="red",xlim=c(1,11),ylim=c(0,0.15),xlab="K-Folds",ylab="Error",main=paste(i,"Samples"))
                par(new=T)
                plot(x,1-TestError,col="blue",axes=F,ann=F,type="b",ylim=c(0,0.15),xlim=c(1,11))
                par(new=F)
                }
        
        }
make.cv_plots.for.samples.range(raw.data)
```
<p style='text-align: justify;'> How many K-folds needed to reliably predict the accuracy of the test error? I have investigated the averaged test error depending on a set of K-folds in range from 2 to 10 for a different numbers of the samples: 500,1000, 5000 and15000 samples.The test error for 500 samples suffers from high variance since the model overfitted the training set and have the largest test error among all tested samples. 1000 sample case is still suffering from high variance. The test error is less than in 500 sample case and reached some saturation at 5%.Using 5000 samples improved the test error significantly and reaches some saturation around 0.0473 (blue line). This case reveals low variance and low bias, suggesting that 5000 samples are sufficient enough to reliable estimate test error and use this amount of samples to predict a new data set. Using almost all available data samples gives the same test errors with near the same value of a minimum, around 0.0470 (or 4.7%), but it takes more time to train the model. Another interesting observation is that K-folds cross-validation works well with a small amount of data. For example, if an experimental data are short of samples then K=10 does much better job compare to K=5 for 500 sample case and it will improve the final outcome of this case with a given model. The conclusion is that to achieve good test error one should use more than 5000 samples with more or equal K>=5.</p>

##Correlation and Principal Component Analysis: 
<p style='text-align: justify;'> 
Having analysed the summary of glm model, we can find that there are some coefficients that have high p-value, high enough to consider them as not significant, suggesting that some variables may be correlated.</p>
```{r echo=FALSE, comment=FALSE, warning=FALSE}
return.glm.summary
```
<p style='text-align: justify;'> 
If there is a correlation among variables, then the estimated accuracy error on test set will tend to overestimate the true test errors. Indeed, the analysis of correlated variables shows that there are 7 pairs of positively correlated variables with correlation coefficients more than 0.9.</p>
```{r echo=FALSE}

data.correlation<-function(processed.data.nofactors){
        recived.cor<-cor(processed.data.nofactors)
        recived.cor[lower.tri(recived.cor)] <- 0
        diag(recived.cor)<-0
        recived.variables<-which(recived.cor>=0.9,arr.ind=T,useNames=T) 
        temp.unique.columns<-c(recived.variables[,1],recived.variables[,2])
        temp.unique.val<-unique(temp.unique.columns)
        temp.data.frame<-(processed.data.nofactors[,c(temp.unique.val)])
        
        temp<-temp.data.frame
        return(list(temp.data.frame=temp.data.frame,recived.variables=recived.variables))
        }

corelated.var<-data.correlation(dummy.num.data[[1]])
corelated.var[[2]]
```
<p style='text-align: justify;'> To reduce the influence of correlation among variables the Principal Component Analysis was used. This approach allows to convert a set of samples of linearly correlated variables into a principal components (or variables) that are linearly uncorrelated.There are 27 principal components that are required to cover 95% of the variance in the data. The results show that accuracy test error of our model became higher when using principal components compare with control (blue vs red line).</p>
```{r echo=FALSE,comment=FALSE, warning=FALSE}

DeteidexofNas<-function(data){ 
        #detect the idecies (df_Ind) and names (df) of colums where NA's are of more than 80 precent
        df<-c()
        df_Ind<-c()
        for (i in 1:dim(data)[2]) {
                if ((length(which(!is.na(data[,i])))/length(data[,i]))<0.8)
                        {df<-cbind(df,names(data)[i])
                         df_Ind<-cbind(df_Ind,i)
                         } 
                } 
        
        return(df_Ind)
        }

ChangeClassforMatrix<-function(data,StartPos,EndPosition,NameOfTheClassToChangeTo){ 
        for (i in StartPos:EndPosition) {
                if (NameOfTheClassToChangeTo=="numeric") {data[,i]<-as.numeric(data[,i])}
                if (NameOfTheClassToChangeTo=="factor") {data[,i]<-as.factor(data[,i])}
                } 
        return(data)
        }


data.cleaning<-function(mydata,number.sampled.data){
        
        #removed variables containing more than 80% of NA's
        pp<-DeteidexofNas(mydata)
        ppst1<-mydata[,-pp]
        
        #No Zeroes Variance Variable
        nzv <- nearZeroVar(ppst1)          
        ppst2<-ppst1[,-nzv]
        
        #No Zeroes, complete samples
        ppst3<-subset(ppst2,ppst2$pitch_forearm!=0) 
        ppst4<-ppst3[,-c(2,5)]           
        
        #No Characters
        temp=dim(ppst4)[2]
        ppst5<-ChangeClassforMatrix(ppst4,1,temp-1,"numeric")    
        
        #make all columns as numerical
        temp0=dim(ppst5)[2]
        ppst5[,temp0]<-as.factor(ppst5[,temp0])      
        
        #make the last columns as factor
        temp1=dim(ppst5)[2]
        ppst6<-preProcess(ppst5[,-temp1],method=c("scale","center")) #Scale and Center
        ppst7<-predict(ppst6,ppst5[,-temp1])
        
        #Scale and Center
        ppst7[, "classe"] <- ppst5$classe
        
        
        dd1<-ppst7[sample(nrow(ppst7),number.sampled.data),]
        
        return(dd1) #No Correaltion on the data 
        
        
        }

CVoneVsAll<-function(data,onefactor,nameFactor,K,EndValueofNumerofVariable,StartValueofNumerofVariable=1){
        
        TrainAcuracy<-vector()
        TestAcuracy_ts<-vector()
        
        TempMatrix<-cbind(data,onefactor)
        EndCol<-dim(TempMatrix)[2]
        EndDataCol<-EndValueofNumerofVariable
        colnames(TempMatrix)[EndCol]<-nameFactor
        
        modelFormula<-AddNamesOfColl(data,StartValueofNumerofVariable,EndDataCol)
        UpdateFit<-paste(colnames(TempMatrix)[EndCol],"~",modelFormula)
        
        flds <- createFolds(TempMatrix[,EndCol], k = K, list = TRUE, returnTrain = T) #make K folds for corss valdation set 
        
        #preform cross validation for range of diffferent K folds starting from 1 to K
        for (i in 1:K) { 
                
                #make training set
                training<-TempMatrix[flds[[i]],] 
                
                #make testing set
                testing<- TempMatrix[-flds[[i]],] 
                
                #fit the glm model for data set, one dummy column and K folds 
                glm.fit<-glm(UpdateFit,data=training,family=binomial) 
                
                # predition on training set
                glm.probs_x<-predict(glm.fit,newdata=training,type="response")
                
                # making probabilities as binary outcome 
                glm.pred_Train_x<-ifelse(glm.probs_x>0.5,"1","0") 
                
                TrainTable<-table(glm.pred_Train_x,training[,EndCol])
                TrainAcuracy<-cbind(TrainAcuracy,(TrainTable[1,1]+TrainTable[2,2])/sum(TrainTable))
                
                # predition on testing set
                # making probabilities as binary outcome 
                glm.probs_ts<-predict(glm.fit,type="response",newdata=testing) 
                glm.pred_ts<-ifelse(glm.probs_ts>0.5,"1","0")        
                
                
                TestTable<-table(glm.pred_ts,testing[,EndCol])
                # comperaing testing factor with predicted on testing set using table function
                #print(TestTable[2,2])
                TestAcuracy_ts<-cbind(TestAcuracy_ts,(TestTable[1,1]+TestTable[2,2])/sum(TestTable)) 
                
                }
        CVoneVsAll<-list(TrainAcuracy<-mean(TrainAcuracy),TestAcuracy_ts<-mean(TestAcuracy_ts))
        return(CVoneVsAll)
        
        }


CossValidation<-function(data, factors, K){ 
        
        # K- cross validation with glm starting from fold=2 to K.K will be sent to CVoneVsAll function
        TrainlistNames<-list()
        TestlistNames<-list()
        
        TrainError<-vector()
        TestError<-vector()
        
        NumbrOfVaibale<-dim(data)[2]    
        DimFactors<-dim(factors)[2]
        x=seq(from=2,to=K,by=1) # x vector of K's
        
        for (d in 2:K){ # do cross validation for range of folds numbers K starting from 2
                
                TrainErrortemp<-vector()
                TestErrortemp<-vector()
                TempMatrix<-data.frame() 
                
                for (i in 1:DimFactors){ 
                        # do cross validation for each of the dummy factors column
                        
                        TempMatrix<-cbind(data,factors[i]) #merge data and factor column "A" (i=1), B(i=2),..) 
                        TrainErrortemp<-cbind(TrainErrortemp,CVoneVsAll(data,factors[i],names(factors)[i],d,NumbrOfVaibale)[[1]] )
                        #train Error for every of Factors colmun for "A" (i=1), "B" (i=2),..) 
                        
                        
                        TestErrortemp<-cbind(TestErrortemp,CVoneVsAll(data,factors[i],names(factors)[i],d,NumbrOfVaibale)[[2]])
                        #test Error for every of Factors colmun for "A" (i=1), "B" (i=2),..) 
                        
                        }
                
                TrainlistNames<-c(TrainlistNames,TrainErrortemp)
                TestlistNames<-c(TestlistNames,TestErrortemp)
                
                TrainError<- cbind(TrainError,mean(TrainErrortemp)) #get the mean value all train errors of "A" (i=1), "B" (i=2),..
                TestError<- cbind(TestError,mean(TestErrortemp)) #get the mean value all test errors of "A" (i=1), "B" (i=2),..
                
                }
        
        
        return(list(TrainError= TrainError,TestError=TestError,x=x))
        }


data.correlation<-function(processed.data.nofactors){
        recived.cor<-cor(processed.data.nofactors)
        recived.cor[lower.tri(recived.cor)] <- 0
        diag(recived.cor)<-0
        recived.variables<-which(recived.cor>=0.9,arr.ind=T,useNames=T) 
        temp.unique.columns<-c(recived.variables[,1],recived.variables[,2])
        temp.unique.val<-unique(temp.unique.columns)
        temp.data.frame<-(processed.data.nofactors[,c(temp.unique.val)])
        par(mfrow=c(1,2),pin=c(2,2))
        
        #pairs(temp.data.frame[,1:3])
        
        temp<-temp.data.frame
        
        plot(temp[,1],temp[,2],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),)
        par(new=T)
        plot(temp[,1],temp[,3],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),col="blue")
        par(new=F)
        
        plot(temp[,2],temp[,3],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),col="red")
        par(new=T)
        plot(temp[,2],temp[,5],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),col="green")
        
        # par(new=T)
        # plot(temp[,7],temp[,13],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),col="darkblue")
        return(list(temp.data.frame=temp.data.frame,recived.variables=recived.variables))
        }

AddNamesOfColl<-function(dat,start,end){ 
        tem<-as.character()
        namesOfVaribales<-names(x = dat)
        for (i in start:(end-1)) {tem<-paste(tem,namesOfVaribales[i],"+")}
        tem<-paste(tem,namesOfVaribales[end])
        return((tem))
        }


#corelated.var<-data.correlation(dummy.num.data[[1]])
#corelated.var[[2]]

my.dummy.variable<-function(data,IndexColToBeDummy){ #make dummy variables from one factor column 
        
        class.name<-sapply(data,class)
        index.classVar<-which(class.name=="factor") #index of factor columns
        temp.vector<-data[,index.classVar] #get the factor colum and assing it to vector
        if (index.classVar!=IndexColToBeDummy) stop("type correct column to be dummied")     
        
        UniqueLevels<-unique(data[,index.classVar]) #unique levels(velues) in factor colunm
        data<-data[,-index.classVar]
        tem<-data.frame()
        tem1<-data.frame()
        tem2<-data.frame()
        
        for (i in 1:length(UniqueLevels)) { #create a dummy varibale matrix
                rr<-temp.vector==UniqueLevels[i]
                rr1<-as.numeric(rr)
                #rr1<-as.factor(rr1)
                tem<-rbind(tem,rr1)
                }
        tem1<-t(tem)
        tem2<-data.frame(tem1)
        names(tem2)<- UniqueLevels #assingin names for colums
        data<-data[,-index.classVar]
        #done<-cbind(data,tem,deparse.level=0)
        
        return(list(Num.data=data,DummyVariables=tem2))
        }

Error_vs_NumberofVariables<-function(cc1,sample.number){
        #make a final graph for 
        #cc1-cleaned data with one multilcass factor column variables at the end. 
        #called CossValidation function(prepresented above)
        temp.ectorOfColums<-vector()                
        EndDummyCol<-dim(cc1)[2]
        
        dd1<-cc1[sample(nrow(cc1), sample.number),]
        dd1[,EndDummyCol]<-as.factor(dd1[,EndDummyCol])
        
        #get a random sample of data
        dd2<-my.dummy.variable(dd1,EndDummyCol)
        dd2_data<-dd2[[1]]
        dd2_factors<-dd2[[2]]
        dd2_factors<-ChangeClassforMatrix(dd2_factors,1,5,"factor") #make all dummy-class varibales as factors
        
        temp.numberts<-vector()
        temp.numbertr<-vector()
        x=seq(from=20,to=dim(dd2_data)[2],by=2)        	 
        
        for (i in x) {						#for loop for testing error as function of the number of variables (represented by vector x) 
                
                temp.VectorOfColums<-dd2_data[1:i]
                tr<-CossValidation(temp.VectorOfColums,dd2_factors,3) #10 folds cross validation 
                EndValTs<-length(tr$TestError)				#getting the mean for TestError 
                temp.numberts<-cbind(temp.numberts,max(tr$TestError))
                EndValTr<-length(tr$TrainError)
                temp.numbertr<-cbind(temp.numbertr,min(tr$TrainError)) #getting the mean for TrainError 
                }
        s<-list(TrainError=temp.numbertr,TestEror=temp.numberts,x=x)
        
        
        #return(list(TrainError=temp.numbertr,TestEror=temp.numberts))
        return(s)
        }

pca.data.cleaning<-function(raw.data,number.sampled.data){
        
        mydata<-raw.data
        #removed variables containing more than 80% of NA's
        pp<-DeteidexofNas(mydata)
        ppst1<-mydata[,-pp]
        
        #No Zeroes Variance Variable
        nzv <- nearZeroVar(ppst1)          
        ppst2<-ppst1[,-nzv]
        
        #No Zeroes, complete cases
        ppst3<-subset(ppst2,ppst2$pitch_forearm!=0) 
        ppst4<-ppst3[,-c(2,5)]           
        
        #No Characters
        temp=dim(ppst4)[2]
        ppst5<-ChangeClassforMatrix(ppst4,1,temp-1,"numeric")    
        
        end.col<-dim(ppst5)[2]
        pca<-preProcess(ppst5[,-end.col],method="pca")
        #pca <- prcomp(ppst5[,-end.col],center = TRUE, scale. = TRUE)
        transformedData <- predict(pca, ppst5[,-end.col])
        transformedData<-as.data.frame(transformedData)
        set.seed(1)
        transformedData[, "classe"] <- ppst5$classe
        dd1<-transformedData[sample(nrow(transformedData),number.sampled.data),]   
        
        return(dd1) #No Correaltion on the data 
        
        }


correlation.pca.data.cleaning<-function(ppst7) {
        endcol<-dim(ppst7)[2]
        ppst8_1<-abs(cor(ppst7[,-endcol]))               
        ppst8_2<-findCorrelation(ppst8_1,cutoff = 0.9)
        ppst8<-ppst7[,-ppst8_2]
        return(ppst8) 
        }

make.correlation.plots<-function(cleaned.data) {
        cleaned.data<-data.cleaning(raw.data)
        nocorrelated.var.data<-correlation.pca.data.cleaning(cleaned.data)
        sample.test1<-Error_vs_NumberofVariables(nocorrelated.var.data,6000)
        x<-sample.test1[[3]]
        return(list(x=x,test.error=1-sample.test1[[2]]))
        
        }



make.pca.plots<-function(raw.data){
        pca.data.temp<-pca.data.cleaning(raw.data,6000)
        sample.test2<-Error_vs_NumberofVariables(pca.data.temp,6000)
        x<-sample.test2[[3]]
        return(list(x=x,test.error=1-sample.test2[[2]]))
        }


ebat<-function(raw.data,cleaned.data) {
        par(mfrow=c(1,1))
        
        cor<-make.correlation.plots(cleaned.data)
        
        plot(cor[[1]],cor[[2]],type="b",col="green",xlim=c(20,56),
             ylim=c(0.04,0.12),xlab="The Number of Values",ylab="Test Error",
             )
        
        par(new=T)
        
        pca<-make.pca.plots(raw.data)
        
        plot(pca[[1]],pca[[2]],type="b",col="blue",xlim=c(20,56),
             ylim=c(0.04,0.12),xlab="The Number of Values",ylab="Test Error")
        par(new=T)
        
        plot(accomplished.plot[[1]],accomplished.plot[[2]],type="b",col="red",xlim=c(20,56), ylim=c(0.04,0.12),axes=F,ann=F)
        
        text(c(42,42,42),c(0.11,0.105,0.1),labels=c("PCA","Control","Without correlated variables"),col=c("blue","red","green"),adj=c(0,0,0))    
        
        #return(list(psa=pca,cor=cor))
        
        }

ebat(raw.data,cleaned.data)
```
<p style='text-align: justify;'> Digging deeply into the data, we can find that, the variables "roll_belt" (5), "yaw_belt"(7), "total_accel_belt"(8), "gyros_dumbbell_z"(13), "accel_belt_y"(40) and  "gyros_forearm_z"(53) are all correlated with each and can be rearranged into a group of correlated variables so that their data points overlap on the graph (left and right panels). And we can see from the figure below, that even those parameters are correlated, the correlation is not subjected to the linear relation (green and red data points, right panel), but rather can be approximated by a higher degree polynomial function.Removal of correlated variables slightly increased the final test error from 4.7% to 6.0 % (green line, figure above). </p>
```{r echo=FALSE, comment=FALSE, warning=FALSE}
plot.data.correlation<-function(temp.data.frame){
        temp<-temp.data.frame
        par(mfrow=c(1,2),pin=c(2.5,2))
        plot(temp[,1],temp[,2],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),main="Variables 7 and 8",xlab="Variable 5",cex.axis=0.9,cex.lab=0.9,ylab="")
        
        
        par(new=T)
        plot(temp[,1],temp[,3],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),col="blue",ann=FALSE,axes=FALSE)
        par(new=F)
        
        plot(temp[,2],temp[,3],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),col="red",main="Variables 8 and 13",xlab="Variable 7",cex.axis=0.9,cex.lab=0.9,ylab="")
        par(new=T)
        plot(temp[,2],temp[,5],xlim=c(-2.5,2.5),ylim=c(-2.5,2.5),col="green",ann=FALSE,axes=FALSE)
        }
plot.data.correlation(corelated.var[[1]])

```

##Conclusions: 

<p style='text-align: justify;'> Based on the performed investigation, we can make conclusions that in order to solve multi-class logistic regression problem and reliably predict a new data set using proposed generalized linear  model (glm) we should train the model on no less than 5000 observations (samples), with K-folds equal more than 5 folds. The analysis shows that there are 7 pairs of  correlated variables that may overestimate the final accuracy test error. It would be also reasonable to remove correlated variables or adjust the linear formula model by replacing the linear sum of correlated variables with higher degree polynomial function. Principal component analysis can be performed at the expense of  the higher test error and it would be reasonable to use only for the sake of faster performance.</p>
```{r include=FALSE, comment=FALSE, warning=FALSE}
variables.vs.cases<-function() { #three option of error_vs_number_of_variables
        #curve for different nunber of cases 
        par(mfrow=c(1,1))
        sample.test1<-Error_vs_NumberofVariables(cleaned.data,500)
        x<-sample.test1[[3]]
        plot(x,1-sample.test1[[2]],type="b",col="blue",xlim=c(19,56),ylim=c(0.04,0.12),xlab="The Number Of Variables",ylab="Test Error",main="Variables is The Tunning Parameter",cex.lab=1, cex.axis=1,cex=1,cex.main=1 )
        
        par(new=TRUE)
        sample.test2<-Error_vs_NumberofVariables(cleaned.data,1000)
        plot(x,1-sample.test2[[2]],col="red",cex=1,axes=F,ann=F,type="b",xlim=c(19,56),ylim=c(0.04,0.12))
        
        par(new=TRUE)
        sample.test3<-Error_vs_NumberofVariables(cleaned.data,5000)
        plot(x,1-sample.test3[[2]],col="green",cex=1,lwd=2,axes=F,ann=F,type="b",xlim=c(19,56),ylim=c(0.04,0.12))
        
        text(c(47,47,47),c(0.115,0.108,0.101),labels=c("500 samples","1000 samples","5000 samples"),col=c("blue","red","green"), adj=c(0,0,0))
        
        par(new=F)
        return(list(x,1-sample.test3[[1]]))
        }
accomplished.plot<-variables.vs.cases()
```

##Predictions:
<p style='text-align: justify;'> 
Predictions made on 20 samples dataset gave the following values: 
.</p>

```{r echo=FALSE, comment=FALSE, warning=FALSE}
predictions.function<-function(data,dummyvar,new.data,Cv_Value){
        #how Error depens on the number of samples?
        #glm: fitting of data with factor vector dummyvar containing n colums of factors for multi-class classification problem.
        #Cv_Value, presentage of data suppose to be Training set, a "p" number in createDataPartition
        dd1<-data #[sample(nrow(data), 6000),]
        
        dummyVarlenght<-dim(dummyvar)[2]
        dataEndVal<-dim(dd1)[2]
        modelFormula<-AddNamesOfColl(dd1,1,dataEndVal)
        TrainAcuracy<-vector()
        TestAcuracy_ts<-vector()
        our.table<-matrix(nrow=5,ncol=20)
        
        for (i in 1:dummyVarlenght) {
                TempMatrix<-cbind(dd1,dummyvar[i])
                indx<-dim(TempMatrix)[2]
                dataEndVal1<-dim(TempMatrix)[2]
                
                inTrain<-createDataPartition(y=TempMatrix[,indx],p=Cv_Value,list=FALSE)
                training<-TempMatrix[inTrain,]
                testing<-TempMatrix[-inTrain,]
                
                UpdateFit<-paste(names(training[dataEndVal1]),"~",modelFormula)
                glm.fit<-glm(UpdateFit,data=training,family=binomial)
                glm.training<-glm(UpdateFit,data=training,family=binomial)
                
                glm.summary<-glm.fit
                
                glm.probs_x<-predict(glm.fit,newdata=training,type="response")
                glm.pred_Train_x<-ifelse(glm.probs_x>0.5,"1","0")
                CFglm.pred_Train_x<-confusionMatrix(glm.pred_Train_x,training[,indx])
                TrainAcuracy<-cbind(TrainAcuracy,CFglm.pred_Train_x[[3]][[1]])
                
                glm.probs_ts<-predict(glm.training,type="response",newdata=testing)
                glm.pred_ts<-ifelse(glm.probs_ts>0.5,"1","0") 
                CFglm.pred_ts<-confusionMatrix(glm.pred_ts,testing[,indx]) 
                TestAcuracy_ts<-cbind(TestAcuracy_ts,CFglm.pred_ts[[3]][[1]])
                
                glm.probs_ts<-predict(glm.training,type="response",newdata=new.data)
                glm.pred_ts<-ifelse(glm.probs_ts>0.5,paste(names(dummyvar)[i]),"0")
                
                our.table[i,]<-(glm.probs_ts)
                
                
                }
        our.table1<-t(our.table)
        our.table1<as.data.frame(our.table1)
        colnames(our.table1)<-names(dummyvar)
        ap<-apply(our.table1,1,max)
        tt<-(our.table1==ap)
        tem.vec<-vector()
        
        endcol<-dim(tt)[1]
        for (i in 1:endcol){
                ind<-which.max(our.table1[i,])
                
                our.table1[i,names(ind)]<-names(ind)
                tem.vec<-cbind(tem.vec,names(ind))
                }
        
        
        
        return(list(our.table1=our.table1,tem.vec=as.character(tem.vec)))
        
        
        }

prediction<-predictions.function(dummy.num.data[[1]],dummy.num.data[[2]],new.dataa,0.3)
prediction[[2]]

```

